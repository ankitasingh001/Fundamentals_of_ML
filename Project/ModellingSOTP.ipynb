{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ModellingSOTP.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "cab-fL1Ra47l",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "outputId": "7822768a-42e6-43fc-8220-184a256f70da"
      },
      "source": [
        "#Operating via google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "root_dir = \"/content/gdrive/My Drive/\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJpIYUpkbhud"
      },
      "source": [
        "#Copying content created previously via pre processing (20%data)\n",
        "!cp /content/gdrive/My\\ Drive/ML\\ Project/ProcessedData_20p_new.csv /content"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-d_0lkIbusF"
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6zziUL18bxNm"
      },
      "source": [
        "col_Names=[\"Sequence\", \"TitleBody\",\"Tags\"]\n",
        "data = pd.read_csv('/content/ProcessedData_20p_new.csv', encoding=\"utf-8\",names=col_Names,header=None)\n",
        "data = data[1:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhCz5PuAcI8y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2a574e06-a522-4632-af35-cb4f35952872"
      },
      "source": [
        "#Considering only 0.5 of 20% of entire data as google colab hangs after 15%\n",
        "data = data.sample(frac=0.0001) \n",
        "data.head()\n",
        "data.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tv3boIRucnWH"
      },
      "source": [
        "\n",
        "df = data\n",
        "#df.head()\n",
        "df.reset_index(drop=True, inplace=True)\n",
        "#df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5UFQPl4eWba"
      },
      "source": [
        "  #Import libraries for encoding \n",
        "  from sklearn.feature_extraction.text import CountVectorizer\n",
        "  import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fzeKH8EBfCa2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7752d233-b90f-4f27-87fd-254781225c0f"
      },
      "source": [
        "#Converting tags to binary one hot encoded tags\n",
        "vector = CountVectorizer(tokenizer = lambda x: x.split(), binary='true')\n",
        "binary_y = vector.fit_transform(df['Tags'])\n",
        "print(binary_y.toarray().shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60, 140)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3MYPouug71J"
      },
      "source": [
        "# we changed considered tags as per the dataset considered\n",
        "# considered tags = top listed tags as per frequency \n",
        "ConsideredTags = 40\n",
        "freq = binary_y.sum(axis=0).tolist()[0]\n",
        "length = len(freq)\n",
        "\n",
        "#sorting tags in descending order of occurance\n",
        "sortedTags = sorted(range(length), key=lambda k: freq[k], reverse=True)\n",
        "#Filtered binary tags excluding less occuring tags\n",
        "filteredTags = binary_y[:,sortedTags[:ConsideredTags]]\n",
        "#print(filteredTags.toarray())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmMmHnsnj8a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "b38fff28-301f-405c-a8b2-eed36a5f326c"
      },
      "source": [
        "sumTags = filteredTags.sum(axis=1)\n",
        "questions_with_no_tags =(np.count_nonzero(sumTags==0))\n",
        "print(\"No of questions which won't be having tags:\",questions_with_no_tags)\n",
        "print(\"Percentage:\",questions_with_no_tags*100/df.shape[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No of questions which won't be having tags: 18\n",
            "Percentage: 30.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0c6wQeDoK58"
      },
      "source": [
        "# Spliting data test:train = 20:80\n",
        "x_train=df.head(int(0.80*df.shape[0]))\n",
        "x_test=df.tail(df.shape[0] - int(0.80*df.shape[0]))\n",
        "\n",
        "y_train = filteredTags[0:int(0.80*df.shape[0]),:]\n",
        "y_test = filteredTags[int(0.80*df.shape[0]):df.shape[0],:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NgdVnDiTsNUI"
      },
      "source": [
        "#Deleting tags and sequence columns as they are not required , only TitleBody and tags encoded are required\n",
        "del x_train['Tags']\n",
        "del x_train['Sequence']\n",
        "\n",
        "del x_test['Tags']\n",
        "del x_test['Sequence']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQtdXeRTsqCY"
      },
      "source": [
        "Now that we obtained our data in the required format we fit the data in bag of words and TfIdf (Term Frequency Inverse Document Frequency) representations:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KnubfH-ms-ZC"
      },
      "source": [
        "#Represntation of our title + body combo in bag of words form\n",
        "Bagvector= CountVectorizer(min_df=0.00009, max_features=150000, tokenizer = lambda x: x.split(),  ngram_range=(1,2))\n",
        "\n",
        "x_train_bow = Bagvector.fit_transform(x_train['TitleBody'])\n",
        "x_test_bow = Bagvector.transform(x_test['TitleBody'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pz2tQtnuvwoL"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJsrSE2nv5xr"
      },
      "source": [
        "#Represntation of our title + body combo in Term Frequency Inverse Document Frequency (TFIDF) form\n",
        "tfidf_vector = TfidfVectorizer(min_df=0.00009, max_features=150000, smooth_idf=True, norm=\"l2\", tokenizer = lambda x: x.split(), sublinear_tf=False, ngram_range=(1,4))\n",
        "\n",
        "x_train_tf = tfidf_vector.fit_transform(x_train['TitleBody'])\n",
        "x_test_tf = tfidf_vector.transform(x_test['TitleBody'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQ8dB_cKywm9"
      },
      "source": [
        "#Importing required packages for classifiers and accuracy calculation\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import f1_score,precision_score,recall_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9VCTMdQ50AZV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "bb8157ea-fe9f-4c11-b636-de7aa47d8e71"
      },
      "source": [
        "print(\"Dimensions of train data X:\",x_train_bow.shape, \"Y :\",y_train.shape)\n",
        "print(\"Dimensions of test data X:\",x_test_bow.shape,\"Y:\",y_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dimensions of train data X: (48, 2827) Y : (48, 40)\n",
            "Dimensions of test data X: (12, 2827) Y: (12, 40)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AzTSaWrc0SyV"
      },
      "source": [
        "import pickle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0kR_krM0agG"
      },
      "source": [
        "# Saving the data, necesarry when data is large to recover the features when memory limits exceeds\n",
        "with open('x_store.pkl','wb') as f:\n",
        "    pickle.dump((x_train_bow,x_test_bow,y_train,y_test),f)\n",
        "\n",
        "x_train_bow,x_test_bow,y_train,y_test = pickle.load(open('x_store.pkl','rb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ga2RF3Iy04Ss"
      },
      "source": [
        "# There is a scipy bug for sparse matrices, so we have used this to handle that bug\n",
        "x_train_bow.sort_indices()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AptnHprh0-Tf"
      },
      "source": [
        "#function to print the respective values for trained classifier\n",
        "def values(pred, true):\n",
        "\n",
        "  print(\"Accuracy : \",metrics.accuracy_score(true,pred))\n",
        "  print(\"Macro f1 score : \",metrics.f1_score(true, pred, average = 'macro'))\n",
        "  print(\"Micro f1 score : \",metrics.f1_score(true, pred, average = 'micro'))\n",
        "  print(\"Hamming loss : \",metrics.hamming_loss(true,pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "leWHCOl61CPO"
      },
      "source": [
        "Using one vs rest classifier with STOCHASTIC GRADIENT DESCENT with BAG OF WORDS features and LOG LOSS (LOGISTIC CLASSIFIER WITH SGD) :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7RpK81m1R1P",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        },
        "outputId": "cf983d94-e076-426c-dedc-b61783da75a5"
      },
      "source": [
        "LogBOW = OneVsRestClassifier(SGDClassifier(loss='log', alpha=0.00001, penalty='l1',n_jobs=-1), n_jobs=-1)\n",
        "LogBOW.fit(x_train_bow.copy(), y_train)\n",
        "predictions = LogBOW.predict(x_test_bow.copy())\n",
        "\n",
        "values(predictions, y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy :  0.3333333333333333\n",
            "Macro f1 score :  0.041666666666666664\n",
            "Micro f1 score :  0.14814814814814817\n",
            "Hamming loss :  0.04791666666666667\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1439: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
            "  'recall', 'true', average, warn_for)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CgUjLWcP9V3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "outputId": "21d38310-cb1c-472e-e8ad-b7ec7b35bd0d"
      },
      "source": [
        "predictions.toarray()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "       [1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
              "      dtype=int64)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ZcC7cge5cf7"
      },
      "source": [
        "Using one vs rest classifier with STOCHASTIC GRADIENT DESCENT with TERM FREQUENCY features and LOGISTIC CLASSIFIER  :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ASbhKze6G5N"
      },
      "source": [
        "LogTF = OneVsRestClassifier(SGDClassifier(loss='log', alpha=0.001, penalty='l1',n_jobs=-1), n_jobs=-1)\n",
        "LogTF.fit(x_train_tf, y_train)\n",
        "predictions = LogTF.predict(x_test_tf)\n",
        "\n",
        "values(predictions, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RA7GUWDz7RaX"
      },
      "source": [
        "Using one vs rest classifier with GRADIENT DESCENT with BAG OF WORDS features and LOGISTIC CLASSIFIER:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m42IGhD7E7aO"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5N_I3L1E8fw"
      },
      "source": [
        "LogBOW = OneVsRestClassifier(LogisticRegression(penalty='l1',verbose=1,n_jobs=-1), n_jobs=-1)\n",
        "LogBOW.fit(x_train_bow.copy(), y_train)\n",
        "predictions = LogBOW.predict(x_test_bow.copy())\n",
        "\n",
        "values(predictions, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKYPD1LwFMVn"
      },
      "source": [
        "Using one vs rest classifier with GRADIENT DESCENT with TERM FREQUENCY features and LOGISTIC CLASSIFIER:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZulWzpDXFUzZ"
      },
      "source": [
        "LogTF = OneVsRestClassifier(LogisticRegression(penalty='l1',verbose=1,n_jobs=-1), n_jobs=-1)\n",
        "LogTF.fit(x_train_tf, y_train)\n",
        "predictions = LogTF.predict(x_test_tf)\n",
        "\n",
        "values(predictions, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUWIcVZZFeuv"
      },
      "source": [
        "Using one vs rest classifier with STCHASTIC GRADIENT DESCENT with BAG OF WORDS features and SVM CLASSIFIER:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i0fNCLB-JN9W"
      },
      "source": [
        "LogBOW = OneVsRestClassifier(SGDClassifier(loss='hinge', alpha=0.00001, penalty='l1',n_jobs=-1), n_jobs=-1)\n",
        "LogBOW.fit(x_train_bow.copy(), y_train)\n",
        "predictions = LogBOW.predict(x_test_bow.copy())\n",
        "\n",
        "values(predictions, y_test)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNgv2TKuJlhf"
      },
      "source": [
        "Using one vs rest classifier with STOCHASTIC GRADIENT DESCENT with TERM FREQUENCY features and SVM CLASSIFIER:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfMiAmoRJluT"
      },
      "source": [
        "SVMtf = OneVsRestClassifier(SGDClassifier(loss='hinge', alpha=0.001, penalty='l1',n_jobs=-1), n_jobs=-1)\n",
        "SVMtf.fit(x_train_tf.copy(), y_train)\n",
        "predictions = SVMtf.predict(x_test_tf.copy())\n",
        "\n",
        "print_performance(predictions, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDgKsU7RKQLS"
      },
      "source": [
        "CODE for HYPER PARAMETER TUNING of various parameters:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubKLwXoaKo9E"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lnvXNJaxKrYO"
      },
      "source": [
        "# grid containing various alpha (parameter) values\n",
        "param_grid = {\"estimator__C\": [0.0001,0.005,0.001, 0.01, 0.1, 1, 10, 100] }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NSWETU-FKvg2"
      },
      "source": [
        "# We changed the code to find alpha values for various types of classifiers we used in the project \n",
        "lgr = OneVsRestClassifier(LogisticRegression(penalty='l1',n_jobs=-1,),n_jobs=-1)\n",
        "clf = GridSearchCV(lgr, param_grid,cv=3,n_jobs=-1)\n",
        "clf.fit(x_train_bow, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3rKR42D4LDxs"
      },
      "source": [
        "print(clf.best_params_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HUynKN54LJ-q"
      },
      "source": [
        "#Testing the predicted values for the obtained parameter\n",
        "predictions = clf.predict(x_test_bow)\n",
        "values(predictions, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}